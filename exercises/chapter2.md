# 第2章 演習問題の解答

### 演習問題2.1
ワンホット表現における各要素を等しく取り扱う場合には距離が等しくなるので、等しくない取り扱いをすればよい。
例えば、以下のように要素の順番において異なる重み $`w_k > 0`$ を用いて距離を計算すれば、一般に 2 つのワンホット表現（次元 $`n`$）$`d_i, d_j`$ の距離は異なるものとなる。

$$
\mathrm{dist} (d_i, d_j) = \sqrt{\sum_{k = 1}^n w_k d_{i, k} d_{j, k} }
$$

これは定義より明らかに距離の公理（非退化、対称、三角不等式）も満たす。
また、これはマハラノビス距離の対角成分のみを有するものに等しい。
どの位置の要素であるかを区別しているのでそのような状況のみで使い得る距離であるが、このようにワンホット表現でも 2 つのトークン間の距離が異なるものを構築することは可能である。

---

### 演習問題2.2
例として `A「遅くなってごめん」B「やっとね」` という文章を考えよう。
B の「やっとね」は同じ文脈に対して使われているが、待ち侘びていてついに会えたという好意的な意味と解釈することもできるし、長らく待たされていて相手を非難する意味と解釈することもできる。

分布仮説は「単語の意味はその単語が登場する文脈によって形成される」というものであったが、文脈をどこまでと捉えるかについては微妙な点を含んでいる。
この文章の前にも文脈があって、AとBが恋人同士で久しぶりに会うことなどが記述されていれば好意的な意味と解釈できるし、代わりに長く待たされていることへの苛立ちなどが記述されていれば非難の意味と解釈するのが自然になる。
一方で、明確な記述がなくて読み手が好きに解釈できるようになっているかもしれない。
また、大量のデータで学習したテキスト生成モデルであれば、この例の文章だけでも好意的な意味も非難の意味もあり得ることを知っており、学習したデータ分布に基づいてどちらがより尤もらしいかを出力することも可能である。

分布仮説は唯一の数学的定義があるものではなく、具体的に何を指すかは場合によるため、分布仮説の議論をする際にはまずは認識を揃えるとよいだろう。

---

### 演習問題2.3
$`\mathcal{S}_{\texttt{gem}} = \{\texttt{priceless}, \texttt{priceless}, \texttt{jewelry}, \texttt{jewelry}, \texttt{ray}\}, \mathcal{S}_{\texttt{jewel}} = \{\texttt{priceless}, \texttt{priceless}, \texttt{priceless}, \texttt{jewelry}, \texttt{ray}, \texttt{faceting}\}`$ であある。

重複排除をする場合（type 条件）は、$` \mathcal{count} (\texttt{gem}_y) = 3, \mathcal{count} (\texttt{jewel}_y) = 4 `$ で $` \texttt{gem}_y \cap \texttt{jewel}_y = \{\texttt{priceless}, \texttt{jewelry}, \texttt{ray} \} `$ であるので、$`M_y = \frac{3}{3} = 1`$ である。

重複排除をしない場合（token 条件）は、$` \mathcal{count} (\texttt{gem}_k) = 5, \mathcal{count} (\texttt{jewel}_k) = 6 `$ で $` \texttt{gem}_y \cap \texttt{jewel}_y = \{\texttt{priceless}, \texttt{priceless}, \texttt{jewelry}, \texttt{ray} \} `$ であるので、$`M_k = \frac{4}{5} = 0.8`$ である。

---

### 演習問題2.4
原論文を読むと、文脈の定義として以下の 4 つを用いている。

- 文内のすべての単語（本書で用いたもの）。
- Lorge Magazine Count (これは The Teacher's Word Book of 30,000 Words. という単語頻度カウントの文献) に基づいて、特定の頻度範囲に入る文内のすべての内容語（名詞・動詞・形容詞・副詞などの実質的な意味を持つ語）。
- 各文の文法構造を考慮して、テーマ単語に最も近接するすべての内容語。
- テーマ単語と最も密接に関連していると判断されたすべての単語。この場合、単語AとCは、単語Cの出現が強く単語Aの出現を暗示し、その逆も同様であると判断された場合、密接に関連していると見なす（これは人間が判断する）。

これらの定義を用いて、単語ペア間の同義度合いを文脈的な重複からどの程度推測できるか、を定量的に調べている。

- まず、重複測定値（$`M_y, M_k`$）は、ある単語ペアが3.0未満の同義度合いを持つ（中程度または低い同義度）、という帰無仮説を検証するための統計量として使う。この 3.0 は本書における図 2.3 の観測から、これ未満とこれ以上で重複測定値の傾きが顕著に変わるので、ここが一つの基準だろうと考えて設定している。
- 片側検定で、第一種の過誤（帰無仮説が正しいにも関わらずそれを棄却してしまうもの）確率を 1% or 5% で固定して、特定の重複測定値を超える低同義度ペアの割合を考えることで、検定棄却点を求める。
- そして、この検定棄却点を超える既知の高同義度ペアの割合を計算し、この割合を仮説が誤りであるときにそれを棄却する確率の推定値とする。これが検出力である。

より具体的には以下のことをしている。
手順にするとやや複雑であるが、文脈の定義を変えながら、同義度が低いが重複測定値が高くなるペアを調べて、その基準よりも高い重複推定値を持つ同義度が高いペアがどれくらい存在するかで文脈の定義の良し悪しを測ろうというものである。

- 同義度合いの評価により、20組のテーマペアが3.0を超えている（これは人間が判定した同義度合いの評価による結果）。
- 第一種の過誤を推定するためにペアの各単語の文セットが異なるグループの被験者によって書かれたすべてのテーマ単語ペア（576組）を考慮して、上記の 20 組を除いた 556 組の低同義度ペアで 1％ または 5％の割合に対して重複測定値の検定棄却点を求めた。
  - 実験では 65 組のペアだけでしかデータを作っていないのでそれらにしか同義度合いのデータは存在しないが、それ以外のペアは明確に同義度合いが低いことを仮定（これはデータを眺めて妥当であると判断しているものと思われる）
  - 65 組のペア以外には同義度合いのデータはないが、コーパス生成の方法から明らかなように重複測定値は計算することができるのでそれは計算して使う
  - 上記を踏まえ、低同義度ペア 45 組の重複測定値と同義度合いのデータがない 511 組の重複測定値も合わせて重複測定値の降べきで並べて、あとは上位 1% or 5% が含まれる重複測定値を見て検定棄却点を求めればよい
- 20組の高同義度ペアのうち検定棄却点を超えるペアの割合を求めた（これが検出力）

結果は以下の図の通りである（図は [https://dl.acm.org/doi/10.1145/365628.365657](https://dl.acm.org/doi/10.1145/365628.365657) より引用）。
一番上の Unrestricted（これは文脈とは文であるというもの）で、CW & FW の場合を見てみよう。
原論文には FW が Functional Word (機能語: 接続詞・冠詞・助動詞・前置詞・代名詞など) である説明があるので、CW は説明がないが Content Word (内容語: 名詞・動詞・形容詞・副詞) である。
unleveled は前処理なしで、leveled は原形にするなどの前処理をしているものである。
見方として、$`M_y`$ の 1% のところは 75% の割合で正しく検出（つまり、20組の高同義性ペアのうち15組が重複測定値に基づいて正しく高同義度であると検出できていて、556組の低同義性ペアのうち 5.56組 つまり 6 組は重複測定値に基づいて誤って高同義度であると判断される）している、と解釈するものである。
括弧内の数字は棄却点における重複測定値の値を書いていて、これは原論文には記載されていないが、0 ~ 1 の値を取るものを百分率として扱うように 100 倍しているものと考えられる。
具体的な数字としては 1% の場合は 34 で、本書の図 2.3 における 0.34 の線に相当する（ただし、図 2.3 は同義度合いのデータがない 511 組はプロットされないので完全に対応はしてないので注意）。

![](/figure/2-4-1.png)

データ数はそこまで多くはないが、全体的な傾向として検出力は一定水準以上あり、定量的にも重複測定値が高同義性の単語を検出する力があることが見て取れる。
文脈としては、より人間の知識を活用したの下の 2 つの場合の方が検出力が高いことも見て取れる。
詳細は置いておくとして、限られたデータ数においては人間の言語知識を活かすことでより性能を高めることができるというのは妥当であろう。

どの文脈の定義も一定の検出力を示しており、データが大量にあれば人間の言語知識を外から与えなくてもモデルが適切に理解するであろうと考え、本書では文脈として「文内のすべての単語」のみを紹介した。

---
